# A = XW + B
# 出力(A)　＝　入力(X)×重み(W)+バイアス(B)
#　ここでは入力、重み、バイアスの数値に意味はない。テキトー

#第１層
#入力データ
X = np.array([1.0,0.5])
#重み
W1 = np.array([[0.1,0.3,0.5],[0.2,0.4,0.6]])
#バイアス
B1 = np.array([0.1,0.2,0.3])

A1 = np.dot(X,W1) + B1

# 活性化関数(h)
# Z = 活性化関数(A)
# 活性化関数にここではシグモイド関数を使用する。
#出力
Z1 = sigmoid(A1)

# 第２層
# 入力X＝第一層の出力 Z1
W2 = np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]])
#第２層のバイアス
B2 = np.array([0.1,0.2])

A2 = np.dot(Z1,W2) + B2
#出力 シグモイド関数
Z2 = sigmoid(A2)

# 第３層
# 恒等関数
def identity_function(x):
    return x

W3 = np.array([[0.1,0.3],[0.2,0.4]])
B3 = np.array([0.1,0.2])

A3 = np.dot(Z2,W3) + B3
#出力
#活性化関数(h)
#　出力層では恒等関数を使用します。
#　恒等関数は入力をそのまま出力する関数です。
Y = identity_function(A3)

# 出力層で使用する活性化関数は解く問題に応じて決定する。
# 回帰問題では恒等関数
# ２クラス分類問題ではシグモイド関数
# 多クラス分類問題ではソフトマックス関数
# を使用するのが一般的
